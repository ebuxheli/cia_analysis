---
title: "Replication of Partners in Crime: An Empirical Evaluation of the CIA Rendition, Detention, and Interrogation Program"
author: "Enxhi Buxheli"
output: pdf_document
bibliography: bibliography.bib
---

## TODO: Add bibliography

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# attaching necessary libraries
library("foreign")
library("gdata")
library("plyr")
library("dplyr")
library("tidyr")
library("ggplot2")
library("stargazer")
library("plm")
library("Amelia")
library("MKmisc")
library("lmtest")
library("janitor")
```

```{r, cache=TRUE}
## Data: Originally there were three datasets used in analysis: 
# 1. "pic_data_not_imputed.RData" - this is the initial dataset before
#     imputation 
# 2. "pic_data_imputed.RData" - this is the initial imputed dataset, contraining
#     observations from 1991-2011 
# 3. "pic_data.RData" - this is the final dataset used in analysis

# Loading in the data created by author of the paper (unable to run the data
# myself because of the 40 hour run time necessary)

# Average of the imputed datasets for plotting (pic_data.RData). 
## This is the only dataset that was used for the creation of the plots in the
## paper. The other .RData files present in the dataverse_files folder are
## unnecessary and remnants of the data pre-processing. For that reason, I will
## only be including this file in my code.
### This replaces the existing data (non-imputed dataset output) variable. Unsure
### of the intent of this by the author, but I don't believe that this is the
### intended outcome.
load("dataverse_files/pic_data.RData") 

# Retrieving number of imputations in the dataset
imps <- length(data$imputations)

# Creating a key of the country codes to be later rematched with the average of
# the imputed data. When the average of the imputed data is taken, the contents
# of the country variable are deleted and this is a way to restore that
# information.

# Creating the country key
data_country <- data$imputations[[1]] %>% 
  distinct(COW, country)

# Average of imputed dataset  
data_analyze <- (data$imputations[[1]] + data$imputations[[2]] + 
                   data$imputations[[3]] + data$imputations[[4]] + 
                   data$imputations[[5]] + data$imputations[[6]] + 
                   data$imputations[[7]] + data$imputations[[8]] + 
                   data$imputations[[9]])/imps

# Joining the datasets and cleaning up variable names for sanity sake (the
# varying capitalizations of variable names was maddening). This is the
# datset that I use throughout the paper in order to create the graphs and 
# tables.
data_final <- inner_join(data_country, data_analyze, 
                         by = "COW", suffix = c(".c", ".a")) %>% 
  select(-country.a, country = country.c) %>% 
  clean_names()
```

(@enos_paper)
# Figure 1
```{r fig1, cache=TRUE}
### TODO: FIGURE OUT A BETTER NAMING CONVENTION FOR THE GRAPHS TO MAKE IT EASIER
### TO CODE AND SEE.

# Plotting figure 1 

# Computing global-year averages for relevant variables
trends_1 <- data_final %>% 
  group_by(year) %>% 
  summarise("Physical Integrity Score (CIRI)"      = mean(physint, na.rm = TRUE),
            "Latent Variable Model Score (Fariss)" = mean(latentmean, na.rm = TRUE),
            "Political Imprisonment Score (CIRI)"  = mean(polpris, na.rm = TRUE),
            "Disappearance Score (CIRI)"           = mean(disap, na.rm = TRUE))

# List of variable names
var_names <- c("Physical Integrity Score (CIRI)", 
               "Latent Variable Model Score (Fariss)", 
               "Disappearance Score (CIRI)", 
               "Political Imprisonment Score (CIRI)")

# Formats the data to be graphed and makes the data facet on var_name
graph_data1 <- gather(trends_1, type, value, var_names)  
graph_data1$type <- factor(graph_data1$type, levels = var_names)

# Creates a tibble with the variable names and the plot limits for each of the
# different plots. Also standardizes the year limits.
var_limits1 <- as_tibble(cbind(year = c(rep(1992:2011, times = 4)), 
                              value = c(rep(c(0, 8),    10), 
                                        rep(c(-0.5, 1), 10), 
                                        rep(c(0, 2),    10), 
                                        rep(c(0, 2),    10))))

# Assign variable names
var_limits1$type <- rep(var_names[1:4], times = 1, each = 20) 

# Converts labels to factors to order plot facets 
var_limits1$type <- factor(var_limits1$type, levels = var_names) 

## TOutputting plot for figure 1
ggplot(graph_data1, aes(x = year, y = value)) +
  geom_line() + 
  geom_vline(xintercept = seq(2001, 2005, by = 0.001), 
             colour = "grey", linetype = "solid", 
             alpha = 0.01) +
  geom_blank(data = var_limits1) +
  facet_wrap(~type, ncol = 2, scales = "free") +
  labs(x = "Year", 
       y = "Lower Score = More Abuse; Higher Score = More Respect") +
  theme_bw() + 
  theme(legend.position = "bottom", legend.title = element_blank(), 
        plot.title = element_text(hjust = 0.5, size=14), 
        text = element_text(size = 13, family = "Times"),
        axis.title = element_text(size = 10), axis.text = element_text(size = 10))

# Saving the plot output for the presentation
ggsave(paste0("present/fig1.png"), 
       width = 9, height = 4.5, 
       plot = last_plot(), 
       device = "png", dpi = "retina")
```

# Figure 2
```{r fig2, cache=TRUE}
# Computing global-year averages for democracies and non-democracies for all var_names
trends_2 <- data_final %>% 
  group_by(year, active_d) %>% 
  summarise("Physical Integrity Score (CIRI)"      = mean(physint, na.rm = TRUE),
            "Latent Variable Model Score (Fariss)" = mean(latentmean, na.rm = TRUE),
            "Political Imprisonment Score (CIRI)"  = mean(polpris, na.rm = TRUE),
            "Disappearance Score (CIRI)"           = mean(disap, na.rm = TRUE))

# Formats the data to be graphed and makes the data facet on var_name
graph_data2 <- gather(trends_2, type, value, var_names)
graph_data2$type <- factor(graph_data2$type, levels = var_names) 

# Relabel active_d for legend using a loop
for (i in 1:length(graph_data2$active_d)){
  if (graph_data2$active_d[i] == 0){
    graph_data2$active_d[i] <- "Other States"
  }
  else if (graph_data2$active_d[i] == 1){
    graph_data2$active_d[i] <- "Active Participants"
  }
}

# Construct matrices to call min and max values for y-axis on facet plots
var_limits2 <- as_tibble(cbind(year = c(rep(1992:2011, times = 4, each = 2)),
                               active_d = c(rep(0:1, times = 40)),
                               value = c(rep(c(0,8),    20), 
                                         rep(c(-0.5,1), 20), 
                                         rep(c(0,2),    20), 
                                         rep(c(0,2),    20))))

# Assign variable names
var_limits2$type <- rep(var_names[1:4], times = 1, each = 40)

# Converts labels to factors to order plot facets
var_limits2$type <- factor(var_limits2$type, levels = var_names)


# Create plots
## Figure 2
ggplot(graph_data2, aes(x = year, y = value, group = active_d)) +
  geom_line(aes(linetype = factor(active_d))) + 
  geom_vline(xintercept = seq(2001, 2005, by = 0.001), 
             colour="grey", linetype = "solid", 
             alpha = 0.01) +
  geom_blank(data = var_limits2) +
  facet_wrap(~type, ncol=2, scales = "free") +
  labs(x = "Year", 
       y = "Lower Score = More Abuse; Higher Score = More Respect") +
  theme_bw() + 
  theme(legend.position = "bottom", legend.title = element_blank(), 
        plot.title = element_text(hjust = 0.5, size=14), 
        text = element_text(size = 13, family = "Times"),
        axis.title = element_text(size = 10), axis.text = element_text(size = 10))

# Saving the plot output for the presentation
ggsave(paste0("present/fig2.png"), 
       width = 9, height = 4.5, 
       plot = last_plot(), 
       device = "png", dpi = "retina")
```

# Figure 3
```{r fig3, cache=TRUE}
# Computing global-year averages for democracies and non-democracies for all var_names
trends_3 <- data_final %>% 
  group_by(year, active_d) %>% 
  filter(below_polity == 1) %>%
  summarise("Physical Integrity Score (CIRI)"      = mean(physint, na.rm = TRUE),
            "Latent Variable Model Score (Fariss)" = mean(latentmean, na.rm = TRUE),
            "Political Imprisonment Score (CIRI)"  = mean(polpris, na.rm = TRUE),
            "Disappearance Score (CIRI)"           = mean(disap, na.rm = TRUE))

# Formats the data to be graphed and makes the data facet on var_name
graph_data3 <- gather(trends_3, type, value, var_names)
graph_data3$type <- factor(graph_data3$type, levels = var_names) 

# Relabel active_d for legend using a loop
for (i in 1:length(graph_data3$active_d)){
  if (graph_data3$active_d[i] == 0){
    graph_data3$active_d[i] <- "Other States"
  }
  else if (graph_data3$active_d[i] == 1){
    graph_data3$active_d[i] <- "Active Participants"
  }
}

# Adjust y-axis for Fariss data
var_limits3 <- as_tibble(cbind(year = c(rep(1992:2011, times = 4, each = 2)),
                                   active_d = c(rep(0:1, times = 40)),
                                   value = c(rep(c(0,8),    20), 
                                             rep(c(-1,0.5), 20), 
                                             rep(c(0,2),    20), 
                                             rep(c(0,2),    20))))

# Assign variable names
var_limits3$type <- rep(var_names[1:4], times = 1, each = 40)

# Converts labels to factors to order plot facets
var_limits3$type <- factor(var_limits3$type, levels = var_names)

# Create Plots
# Figure 3
ggplot(graph_data3, aes(x = year, y = value, group = active_d)) +
  geom_line(aes(linetype = factor(active_d))) + 
  geom_vline(xintercept = seq(2001, 2005, by = 0.001), 
             colour="grey", linetype = "solid", 
             alpha = 0.01) +
  geom_blank(data = var_limits3) +
  facet_wrap(~type, ncol=2, scales = "free") +
  labs(x = "Year", 
       y = "Lower Score = More Abuse; Higher Score = More Respect") +
  theme_bw() + 
  theme(legend.position = "bottom", legend.title = element_blank(), 
        plot.title = element_text(hjust = 0.5, size=14), 
        text = element_text(size = 13, family = "Times"),
        axis.title = element_text(size = 10), axis.text = element_text(size = 10))

# Saving the plot output for the presentation
ggsave(paste0("present/fig3.png"), 
       width = 9, height = 4.5, 
       plot = last_plot(), 
       device = "png", dpi = "retina")
```

# Tables
```{r setting_models, results='asis', cache=TRUE}
# This block is devoted to the definition of the different models that are 
# used in the creation of the table for the analysis. The blocks with the tables
# follow after. The general structure of the construction of the models is 
# (1) to intialize and empty data set to be appended to later, (2) setting
# up the formula to be used in the linear model, (3) calculating the model,
# (4) extracting the coefficients for each imputation, (5) aggregating these,
# (6) inputting these numbers into a plm object whcih will be the direct output 
# of the table.

# Creates a function to compute the variance across the imputations. The value
# outputted from this function will be used in the table and will be the
# standard error shown. [se = standard error, betas = estimate coefficients] The
# 2 in the apply function is the margin which means that the function will be
# applied to the columns of whatever the model inputted is.
aggregate_se <- function(betas, se){
  # Calculating the variance for the standard errors
  t_one <- apply(se, 2, 
                 function(x) sum(x^2)/length(x) )
  
  # Calculating the variance for the estimate coefficients
  t_two <- apply(betas, 2, 
                 function(x) sum((x - mean(x))^2 / (length(x) - 1)) * (1 + (1 / length(x))) )
  
  # Adding the variances and returning the total variance
  t_one + t_two
}

# General Function for Model 1
mod_1 <- function(DV, LAG_DV){
  ## Creating an empty matrix to be used to store the future values for the
  ## estimated coefficients (betas), and the standard errors (std.error).
  betas <- matrix(NA, nrow = imps, ncol = 2) # Matrix for estimated coefficients 
  st.errors <- matrix(NA, nrow = imps, ncol = 2) # Matrix for standard errors 
  
  ## This will be a formula used to build the model
  fmla <- formula(paste0(substitute(DV), "~ active_t +", substitute(LAG_DV)))
  
  # Create a linear model and save standard errors for each imputed dataset
  for (i in 1:imps){
    ## plm is a linear model for panel data: will allow us to save standard errors
    ## for the imputed dataset
    mod <- plm(fmla, 
               data=data$imputations[[i]], 
               index=c("COW", "YEAR"), 
               effect ="twoways", model="within") 
    
    # Computing the country level cluster robust standard errors
    mod <- coeftest(mod, vcovHC(mod, type = "HC3", cluster = "group")) 
    
    # Saving the relevant values for each of the imputations: estimate
    # coefficients and standard errors. These will be later aggregated to
    # calculate the values used in the table
    ## Estimate Coefficients
    betas[i,] <- mod[,1]
    ## Standard Errors
    st.errors[i,] <- mod[,2] 
  }
  
  # Aggregated the estimate coefficient and standard error for all of the
  # imputations. This will be the value in the table.
  ## Mean estimate coefficient
  beta_estimates <- colMeans(betas)  
  ## Aggregated standard error: uses the function created above
  crse_estimates <- aggregate_se(betas, st.errors) 
  
  # Inserting the correct coefficient and standard error values into the plm
  # object to be inputted into the table
  ## Creates a placeholder plm object using the first imputation
  mod <- plm(fmla, data = data$imputations[[1]], 
             index=c("COW", "YEAR"), effect ="twoways", model="within")
  
  ## Replacing the values of the plm object
  for(i in 1:length(colMeans(betas))){
    mod$coefficients[[i]] <- beta_estimates[i]
  }
  
  ## Replacing the variances using the diagonal of the plm variance-covariance matrix
  diag(mod$vcov) <- crse_estimates
  
  ## Outputting the model
  mod
}

# General Function for Model 2
mod_2 <- function(DV, LAG_DV){
  ## Creating an empty matrix to be used to store the future values for the
  ## estimated coefficients (betas), and the standard errors (std.error).
  betas <- matrix(NA, nrow = imps, ncol = 3)
  st.errors <- matrix(NA, nrow = imps, ncol = 3)
  
  ## This will be a formula used to build the model
  fmla <- formula(paste0(substitute(DV)," ~ Dem_Part + Auto_Part +", substitute(LAG_DV)))
  
  # Create a linear model and save standard errors for each imputed dataset
  for (i in 1:imps){
    ## plm is a linear model for panel data: will allow us to save standard errors
    ## for the imputed dataset
    mod <- plm(fmla, 
               data = data$imputations[[i]], 
               index=c("COW", "YEAR"), 
               effect ="twoways", model="within")
    
    # Computing the country level cluster robust standard errors
    mod <- coeftest(mod, vcovHC(mod, type = "HC3", cluster = "group"))
    
    # Saving the relevant values for each of the imputations: estimate
    # coefficients and standard errors. These will be later aggregated to
    # calculate the values used in the table
    ## Estimate Coefficients
    betas[i,] <- mod[,1]
    ## Standard Errors
    st.errors[i,] <- mod[,2]
  }

  # Aggregated the estimate coefficient and standard error for all of the
  # imputations. This will be the value in the table.
  ## Mean estimate coefficient
  beta_estimates <- colMeans(betas)  
  ## Aggregated standard error: uses the function created above
  crse_estimates <- aggregate_se(betas, st.errors) 
  
  # Inserting the correct coefficient and standard error values into the plm
  # object to be inputted into the table
  ## Creates a placeholder plm object using the first imputation
  mod <- plm(fmla, data = data$imputations[[1]], 
             index=c("COW", "YEAR"), effect ="twoways", model="within")
  
  ## Replacing the values of the plm object
  for(i in 1:length(colMeans(betas))){
    mod$coefficients[[i]] <- beta_estimates[i]
  }
  
  ## Replacing the variances using the diagonal of the plm variance-covariance matrix
  diag(mod$vcov) <- crse_estimates
  
  ## Outputting the model
  mod 
}

# General Function for Model 3
mod_3 <- function(DV, LAG_DV){
  ## Creating an empty matrix to be used to store the future values for the
  ## estimated coefficients (betas), and the standard errors (std.error).
  betas <- matrix(NA, nrow = imps, ncol = 10)
  st.errors <- matrix(NA, nrow = imps, ncol = 10)
  
  ## This will be a formula used to build the model
  fmla <- formula(paste0(substitute(DV), "~ active_t + ucdp_type3 + gtd + trans + 
                         polity2 + log_pop + log_gdppc + log_UStrade + log_USmilaid +", 
                         substitute(LAG_DV)))
  
  # Create a linear model and save standard errors for each imputed dataset
  for (i in 1:imps){
    ## plm is a linear model for panel data: will allow us to save standard errors
    ## for the imputed dataset
    mod <- plm(fmla, 
               data = data$imputations[[i]], 
               index=c("COW", "YEAR"), 
               effect ="twoways", model="within")
    
    # Computing the country level cluster robust standard errors
    mod <- coeftest(mod, vcovHC(mod, type = "HC3", cluster = "group"))
    
    # Saving the relevant values for each of the imputations: estimate
    # coefficients and standard errors. These will be later aggregated to
    # calculate the values used in the table
    ## Estimate Coefficients
    betas[i,] <- mod[,1]
    ## Standard Errors
    st.errors[i,] <- mod[,2]
  }
  
  # Aggregated the estimate coefficient and standard error for all of the
  # imputations. This will be the value in the table.
  ## Mean estimate coefficient
  beta_estimates <- colMeans(betas)  
  ## Aggregated standard error: uses the function created above
  crse_estimates <- aggregate_se(betas, st.errors) 
  
  # Inserting the correct coefficient and standard error values into the plm
  # object to be inputted into the table
  ## Creates a placeholder plm object using the first imputation
  mod <- plm(fmla, data = data$imputations[[1]], 
             index=c("COW", "YEAR"), effect ="twoways", model="within")
  
  ## Replacing the values of the plm object
  for(i in 1:length(colMeans(betas))){
    mod$coefficients[[i]] <- beta_estimates[i]
  }
  
  ## Replacing the variances using the diagonal of the plm variance-covariance matrix
  diag(mod$vcov) <- crse_estimates
  
  ## Outputting the model
  mod
}

# General Function for Model 4
mod_4 <- function(DV, LAG_DV){
  ## Creating an empty matrix to be used to store the future values for the
  ## estimated coefficients (betas), and the standard errors (std.error).
  betas <- matrix(NA, nrow = imps, ncol = 11)
  st.errors <- matrix(NA, nrow = imps, ncol = 11)
  
  ## This will be a formula used to build the model
  fmla <- formula(paste0(substitute(DV), "~ Dem_Part + Auto_Part + ucdp_type3 + gtd + 
                         trans + polity2 + log_pop + log_gdppc + log_UStrade + log_USmilaid +", 
                         substitute(LAG_DV)))
  
  # Create a linear model and save standard errors for each imputed dataset
  for (i in 1:imps){
    ## plm is a linear model for panel data: will allow us to save standard errors
    ## for the imputed dataset
    mod <- plm(fmla, 
               data = data$imputations[[i]], 
               index=c("COW", "YEAR"), 
               effect ="twoways", model="within")
    
    # Computing the country level cluster robust standard errors
    mod <- coeftest(mod, vcovHC(mod, type = "HC3", cluster = "group"))
    
    # Saving the relevant values for each of the imputations: estimate
    # coefficients and standard errors. These will be later aggregated to
    # calculate the values used in the table
    ## Estimate Coefficients
    betas[i,] <- mod[,1]
    ## Standard Errors
    st.errors[i,] <- mod[,2]
  }
  
  # Aggregated the estimate coefficient and standard error for all of the
  # imputations. This will be the value in the table.
  ## Mean estimate coefficient
  beta_estimates <- colMeans(betas)  
  ## Aggregated standard error: uses the function created above
  crse_estimates <- aggregate_se(betas, st.errors) 
  
  # Inserting the correct coefficient and standard error values into the plm
  # object to be inputted into the table
  ## Creates a placeholder plm object using the first imputation
  mod <- plm(fmla, data = data$imputations[[1]], 
             index=c("COW", "YEAR"), effect ="twoways", model="within")
  
  ## Replacing the values of the plm object
  for(i in 1:length(colMeans(betas))){
    mod$coefficients[[i]] <- beta_estimates[i]
  }
  
  ## Replacing the variances using the diagonal of the plm variance-covariance matrix
  diag(mod$vcov) <- crse_estimates
  
  ## Outputting the model
  mod
}
```

```{r table_setup}
# Defining the characteristics for the stargazer object
# Setting the covariate labels (those on the left-hand side)
covariate_names <- c("Participation", "Democratic participation", "Autocratic participation", 
                     "Internal conflicts", "Terrorist attacks", "Transitional state", 
                     "Polity score", "Log population", "Log GDP per capita", "Log US trade", 
                     "Log US military assistance")

# Setting up the line to show fixed effects
fe_line <- list(c("Fixed effects", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes"))

# Adding a footnote
footnote <- c("All models include country and year fixed effects and a 
              dependent variable lagged one year. Country level",
              "cluster-robust standard errors in parentheses. 
              $^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01")

# Setting the title
title1 <- "Participation in RDI program and state respect for human rights, 1992-2011"
```


```{r table2, results='asis', cache=TRUE}
# Run analysis and produce tables
## Table 1 for our analysis (table 2 in the paper)
stargazer(mod_1(PHYSINT, lag_physint), mod_2(PHYSINT, lag_physint),
          mod_3(PHYSINT, lag_physint), mod_4(PHYSINT, lag_physint),
          mod_1(latentmean, lag_latentmean), mod_2(latentmean, lag_latentmean),
          mod_3(latentmean, lag_latentmean), mod_4(latentmean, lag_latentmean),
          header = FALSE, style = "apsr", title = title1, digits = 3, 
          covariate.labels = covariate_names, add.lines = fe_line, 
          notes = footnote, notes.append = FALSE, notes.align = "l",
          font.size = "small", column.sep.width = "-8pt",
          omit = c("lag_physint", "lag_latentmean"),
          omit.stat = c("adj.rsq", "f"),
          dep.var.labels = c("Physical Integrity Score (CIRI)", 
                             "Latent Variable Model Score (Fariss)"))
```

```{r table3, results='asis', cache=TRUE}
# Run analysis and produce tables
## Table 2 for our analysis (table 3 in the paper)
stargazer(mod_1(DISAP, lag_disap), mod_2(DISAP, lag_disap),
          mod_3(DISAP, lag_disap), mod_4(DISAP, lag_disap),
          mod_1(POLPRIS, lag_polpris), mod_2(POLPRIS, lag_polpris),
          mod_3(POLPRIS, lag_polpris), mod_4(POLPRIS, lag_polpris),
          header = FALSE, style = "apsr", title = title1, digits = 3, 
          covariate.labels = covariate_names, add.lines = fe_line, 
          notes = footnote, notes.append = FALSE, notes.align = "l",
          font.size = "small", column.sep.width = "-8pt",
          omit = c("lag_disap", "lag_polpris"),
          omit.stat = c("adj.rsq", "f"),
          dep.var.labels = c("Disappearance Score (CIRI)", 
                             "Political Imprisonment Score (CIRI)"))
```

# Extension

```{r, include=FALSE, cache=TRUE}
# Conduct a series of difference-of-means tests comparing global human rights
# practices before and after the onset of the RDI program (Part A); and a series
# of difference-of-means tests to check the comparability of participants and
# non-participants before the beginning # of the RDI program (Part B).

# #---------------------------------------------------------------------------------------------
# # Part A. Difference-of-Means Tests of Human Rights Before/After 2001, 2002, 2003, 2004, 2005
# #---------------------------------------------------------------------------------------------
# 
# # Clear workspace and re-load data
# #load("pic_data.RData")
# #data.out <- data
# cutoff_years <- c(2001, 2002, 2003, 2004, 2005) # Creat vectorof relevant cutoff years
# var_names <- c("PHYSINT", "Latent", "State", "Amnesty", "KILL", "POLPRIS", "TORT", "DISAP") # Vector of dependent variable names
# 
# # Function that creates dataset of states' average dependent variable values for before/after cutoff dates  
# average_dv_pre_post <- function(year, df){
#   post_years <- df %>% group_by(COW) %>% filter(year >= year) %>% 
#     summarise(PHYSINT = mean(PHYSINT, na.rm = TRUE), Latent = mean(latentmean, na.rm = TRUE),
#               State = mean(State, na.rm = TRUE), Amnesty = mean(Amnesty, na.rm = TRUE),
#               KILL = mean(KILL, na.rm = TRUE), POLPRIS = mean(POLPRIS, na.rm = TRUE),
#               TORT = mean(TORT, na.rm = TRUE), DISAP = mean(DISAP, na.rm = TRUE))
#   post_years$pre_post <- 1
#   pre_years <- df %>% group_by(COW) %>% filter(year < year) %>% 
#     summarise(PHYSINT = mean(PHYSINT, na.rm = TRUE), Latent = mean(latentmean, na.rm = TRUE),
#               State = mean(State, na.rm = TRUE), Amnesty = mean(Amnesty, na.rm = TRUE),
#               KILL = mean(KILL, na.rm = TRUE), POLPRIS = mean(POLPRIS, na.rm = TRUE),
#               TORT = mean(TORT, na.rm = TRUE), DISAP = mean(DISAP, na.rm = TRUE))
#   pre_years$pre_post <- 0
#   out <- as.data.frame(rbind(pre_years, post_years))
#   out
# }
# 
# data.out.ttest <- data.out # Create object to hold datasets for analysis
# 
# # Run Analysis 
# for(i in 1:length(cutoff_years)){
#   for(j in 1:length(data.out$imputations)){data.out.ttest$imputations[[j]] <- average_dv_pre_post(cutoff_years[i], data.out$imputations[[j]])}
#   cat("\n \n \n \n", "Analysis With Cutoff Year =", cutoff_years[i],"\n \n")
#   for(k in 1:length(var_names)){
#     out <- mi.t.test(data.out.ttest$imputations, x = var_names[k], y = "pre_post", alternative = c("two.sided"), var.equal = FALSE)
#     print(out)
#   }
# }
# # Function that gets country averages for all variables during a specified period
# balance_data <- function(start_year, stop_year, df){
#   out <- df %>% group_by(COW) %>% 
#     filter(year >= start_year & year <= stop_year) %>%
#     summarise(active_d = mean(active_d), PHYSINT = mean(PHYSINT), 
#               Latent = mean(latentmean),State = mean(State), Amnesty = mean(Amnesty),
#               KILL = mean(KILL), POLPRIS = mean(POLPRIS), TORT = mean(TORT), 
#               DISAP = mean(DISAP), ucdp_type3 = mean(ucdp_type3), trans = mean(trans), 
#               log_pop = mean(log_pop), log_gdppc = mean(log_gdppc), polity2 = mean(polity2), 
#               gtd = mean(gtd), log_UStrade = mean(log_UStrade), log_USmilaid = mean(log_USmilaid))
#   out <- as.data.frame(out)
#   out
# }
# data.out.baltest <- data.out # Create object to hold datasets for analysis
# vars <- c("PHYSINT", "Latent", "State", "Amnesty", "KILL", "POLPRIS", "TORT", "DISAP",
#           "ucdp_type3", "trans", "log_pop", "log_gdppc", "polity2", "gtd", "log_UStrade", "log_USmilaid")
# 
# # Run Analysis 
# for(j in 1:length(data.out$imputations)){data.out.baltest$imputations[[j]] <- balance_data(1998, 2000, data.out$imputations[[j]])} # Subset relevant data
# cat("\n \n \n \n", "Balance between Participants and Non-Participants from 1998 until 2000 \n \n") # Print title
# for(k in 1:length(vars)){ # Loop through difference of means tests for all variables and print results
#     out <- mi.t.test(data.out.baltest$imputations, x = vars[k], y = "active_d", alternative = c("two.sided"), var.equal = FALSE)
#     print(out)
# }
```

```{r extension}

```



